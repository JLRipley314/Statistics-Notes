Parts of this chapter are inspired by sections in \cite{zhou2008practical,feller1968introduction}

First we define a few concepts about Markov chains.
There is a large literature on Markov chains. 
Here we discuss only a few aspects of the theory. 
In particular we will only discuss discrete Markov chains--that is Markov chains where that make discrete jumps in time.

We will borrow some of the concepts from the discussion on stochastic processes in Chptr.~\ref{chap:time-series-analysis}.
A \textbf{Markov chain} is a sequence of random vectors (a stochastic process) $\left\{\btheta_1,\btheta_2,...\right\}$, where the probability distribution of $\btheta_{k+1}$ is dependent solely on $\btheta_n$.
That is 
\begin{align}
    P\left(\btheta_{k+1}=\bj|\btheta_{k}=\bi, \btheta_{k-1} = \bi_{k-1}, \cdots \btheta_{0} = \bi_0\right) 
     =
     P\left(\btheta_{k+1}=\bj|\btheta_n=\bi\right)
     .
\end{align}
Here the $\bj,\bi_m$ label the states of the Markov chain--notice that the subscript of $\btheta$ denotes what step we're on in the Markov chain. 
We call $P\left(\btheta_{k+1}=\bj|\btheta_n=\bi\right)$ the \textbf{transition probability}. 
The probability distribution for $\btheta_0$, $P\left(\btheta_0\right)$, is called the \textbf{initial distribution}.

Notice by the conservation of probability, we have
\begin{align}
    \label{eq:conservation-of-probability}
    \sum_{\bj\in S}P\left(\btheta_{k+1}=\bj|\btheta_n=\bi\right)
    =
    1
    ,
\end{align}
where the sum over all states of the Markov chain.
In other words, witha Markov chain, in the next step you must go \emph{somewhere} with probability one--probability must be conserved.


If the transition probabilities are \textbf{stationary}, the transition probabilities do not depend on the step $n$ in the chain.
In that case we sometimes denote the transition probabilities with the simpler notation 
\begin{align}
     P\left(\btheta_{k+1}=\bj|\btheta_n=\bi\right)
     \to
     P\left(\bj|\bi\right)
    .
\end{align}
We will only consider stationary transition probabilities.
%=================================================================================
\section{Representations of Markov chains\label{sec:representations-markov-chains}}

Some references set $P\left(\bj|\bi\right) = p_{\bi\bj}$, others set $P\left(\bj|\bi\right) = p_{\bj\bi}$.
While I find the latter notation more natural, it seems like the most common notation is the first one (e.g. \cite{wasserman2010statistics,pishro2014introduction}), so we pick that one
\begin{align}
    \label{eq:notational-shorthand-makov-step}
    P\left(\bj|\bi\right)
    \equiv
    p_{\bi\bj}
    .
\end{align}
By \eqref{eq:conservation-of-probability}, and \eqref{eq:notational-shorthand-makov-step}, we see that the elements of each row must sum to one
\begin{align}
    \label{eq:rows-sum-to-one}
    \sum_{\bj\in S} p_{\bi\bj}
    =
    1
    .
\end{align}
With Eq.~\eqref{eq:notational-shorthand-makov-step}, the PDF of a chain $\left\{\btheta_0=\bi_0, \btheta_1=\bi_i, ... , \btheta_k=\bi_k\right\}$ is
\begin{align}
    P\left(\left\{\btheta_0=\bi_0, \btheta_1=\bi_1, ... , \btheta_n=\bi_n\right\}\right)
    =
    p_{\bZero} p_{\bZero,\bOne} p_{\bOne,\bTwo} \cdots p_{\bk-\bOne,\bk}
    ,
\end{align}

When the states are scalars, we can set $p_{\bi \bj} \to p_{ij}$ to represent the components of a matrix. 
Sometimes the transition probabilities are represented in terms of a \textbf{state transition matrix} $P$, where the rows/columns are given by
\begin{align}
    P_{ij}
    =
    p_{ij}
    .
\end{align}
Another common way to describe a Markov chain is through a \textbf{transition state diagram} (I find this to be especially useful for solving problems that involve Markov chains).
One draws an arrow between points in the state space that have nonzero transition probabilities between them, and then labels each arrow with the transition probability.

%=================================================================================
\section{Transition probabilities for scalar states\label{sec:transition-probabilities}}

We next turn to a more vectorial representation of the evolution of probability distribution for scalar random variables.
We consider an initial probability distribution of states, which we represent as a row vector 
\begin{align}
    \bpi_0
    =
    \begin{pmatrix}
        P\left(\theta_0=1\right),\cdots,P\left(\theta_0=n\right)
        .
    \end{pmatrix}
    .
\end{align}
The probability distribution of of states for the next step in the Markov chain is 
\begin{align}
    P\left(\theta_1=j\right) 
    =
    \sum_{i\in S} P\left(j|i\right) P\left(\theta_0=i\right)
    .
\end{align}
In vectorial notation, we have
\begin{align}
    \left(\bpi_1\right)_i = \left(\bpi_0\right)_j P_{ji}
    .
\end{align}
Iterating, we see that the probability distribution at the $k^{th}$ step is
\begin{align}
    \bpi_k
    =
    \bpi_0 P^k
    .
\end{align}

Finally, we note that from the law of total probability, we have
\begin{align}
    P\left(\theta_{m+n}=j|\theta_0=i\right)
    =
    \sum_{k\in S} 
        P\left(\theta_{m}=j|\theta_n=k\right)
        P\left(\theta_{n}=k|\theta_0=i\right)
    .
\end{align}
We can rewrite these as follows
\begin{align}
    \label{eq:Chapman-Kolmogorov-eqns}
    p^{(m+n)}_{ij} 
    =
    \sum_{k\in S} p^{(n)}_{ik}p^{(m)}_{kj}
    .
\end{align}
These are called the \textbf{Chapmon-Kolmogorov equations}.
Here we have used the notation 
\begin{align}
    p^{(n)}_{ij}
    \equiv
    P\left(\theta_{n}=j|\theta_0=i\right) 
    =
    p_{ik_1}p_{k_2k_3}\cdots p_{k_nj}
    .
\end{align}

%=================================================================================
\section{Absorbing states  \label{sec:absorbing-states}}

For a given state $\bi$, if there is an integer $n$ such that 
\begin{align}
    P\left(\bX_n = \bi | \bX_0 = \bi\right) = 1
    ,
\end{align}
we say the state $\bi$ is \textbf{recurrent}.
If instead for all $n$ we have 
\begin{align}
    P\left(\bX_n = \bi | \bX_0 = \bi\right) < 1
    ,
\end{align}
we then say that the state is \textbf{transient}.
We say a state $\bi$ is \textbf{absorbing} if
\begin{align}
    \label{eq:absorbing_condition}
    P\left(\bX_1 = \bi | \bX_0 = \bi\right) = 1
    .
\end{align}
The condition \eqref{eq:absorbing_condition} implies that once you are at state $\bi$, we cannot move to any other state.

%=================================================================================
\subsection{Calculating absorption probabilities}
Let $\bj_a$ is an absorbing state. 
The probability that a state $\bi$ will eventually end up at $\bj_a$ is called the \textbf{absorption probability}.
We denote the absorption probability by
\begin{align}
    a_{\bi,\bj_a}
    \equiv
    P\left(\mathrm{absorption\;by\;}\bj_a|\bX_0=\bi\right)
    .
\end{align}
To compute $a_{\bi, \bj_a}$, we first set $a_{\bi, \bk_a}=0$ for any other absorbing states $\bk_a\neq\bj_a$.
This is because for any other absorbing state, clearly we must have
\begin{align}
    a_{\bi,\bk_a}
    = 
    P\left(\mathrm{absorption\;by\;}\bk_a|\bX_0=\bj_a\right)
    =
    \delta_{\bk_a, \bj_a}
    ,
\end{align}
since $P\left(\mathrm{absorption\;by\;}\bj_a|\bX_0=\bj_a\right)=1$ for all absorbing states.
To compute the $a_{\bi,bj_a}$ for all $\bi$ that are not absorbing, we use the law of total probability to derive a set of linear equations
\begin{align}
    \label{eq:linear-eqns-absorbtion-probabilities-derivation}
    a_{\bi,\bj_a}
    =&
    P\left(\mathrm{absorption\;by\;}\bi|\bX_0=\bj_a\right)
    \nonumber\\
    =&
    \sum_{\bk\in S} P\left(\mathrm{absorption\;by\;}\bj_a|\bX_1=\bk\right)P\left(\bX_1=\bk|\bX_0=\bi\right)
    \nonumber\\
    =&
    \sum_{\bk\in S} a_{\bk,\bj_a} P\left(\bk|\bi\right)  
    .
\end{align}
That is we have (for all non-absorbing states)
\begin{align}
    \label{eq:linear-eqns-absorbtion-probabilities}
    a_{\bi,\bj_a}
    =
    \sum_{\bk\in S} p_{\bi\bk}a_{\bk,\bj_a}   
    .
\end{align}
On the last line we made use of the fact that since the transition probabilities are stationary, $a_{\bi,\bj_a}$ does not depends only on $\bi$ and $\bj_a$, that is it does not depend on what step of the chain we are on.
With $a_{\bk_a,\bj_a}=\delta_{\bk_a,\bj_a}$ for the absorbing states, \eqref{eq:linear-eqns-absorbtion-probabilities} defines a set of linear equations that we can solve for the absorption probabilities to $\bj_a$, $a_{\bk,\bj_a}$, for the non-absorbing states.

%=================================================================================
\subsection{Calculating mean absorption times}

In addition to determine the probability of reaching a given absorbing state $\bj_a$, another question of practical importance is determining the expectation of the number of steps it will take to reach \emph{any} absorbing state.
We denote the mean steps by
\begin{align}
    \mathbb{E}\left[\mathrm{steps}\;T\;\mathrm{until\;absorption}|\bi\right]
    =
    \mu_{\bi}
    .
\end{align}
Clearly $\mu_{\bj_a}=0$ for all absorbing states $\bj_a$.
To compute $\mu_{\bi}$ for non-absorbing states $\bi$, we take one step, and use the law of total probability (iterated expectation)
\begin{align}
    \label{eq:linear-eqns-absorbtion-steps-derivation}
    \mu_{\bi}
    =&
    \mathbb{E}\left[\mathrm{steps}\;T\;\mathrm{until\;absorption}|\btheta_0=\bi\right]
    \nonumber\\
    =&
    1
    +
    \sum_{\bj} \mathbb{E}\left[\mathrm{steps}\;T\;\mathrm{until\;absorption}|\btheta_1=\bj\right]
                P\left(\btheta_1=\bj|\btheta_0=\bi\right)
    \nonumber\\
    =&
    1
    +
    \sum_{\bj} \mu_{\bj}P\left(\bj|\bi\right)
    .
\end{align}
That is we have (for all non-absorbing states)
\begin{align}
    \label{eq:linear-eqns-absorbtion-steps}
    \mu_{\bi}
    =
    1
    +
    \sum_{\bj} p_{\bi\bj}\mu_{\bj}
    .
\end{align}
Equation~\eqref{eq:linear-eqns-absorbtion-steps} defines a set of linear equations for the expectation of the absorption time.
Note that this is the expectation to be absorbed by \emph{any} state, as opposed to \eqref{eq:linear-eqns-absorbtion-probabilities}, which defines the probability to be absorbed by a \emph{specific} state $\bj_a$. 
