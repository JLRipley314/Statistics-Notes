In the Bayesian approach to statistics, you assign probabilities to your hypothesis.
By contrast, in frequentist statistics you assign probabilities to the data you collect.
For example, in Bayesian statistics you would ask ``Given the data I've collected, how probable are these model parameters?''.
In frequentists statistics, you ask ``given this assumption for the model parameters, how probable is it I would have collected this data?''.
Most of these notes cover more Bayesian-like statistics, but neither approach is inherantly better than the other.
Here I review some concepts that appear more often in frequentist approaches to statistics.
Wasserman provides an excellent review of frequentist statistics \cite{wasserman2010statistics}.

%--------------------------------------------------------------------
\section{Statistical estimators}


For this reason, frequentist statistical methods often revolve around \textbf{statistical estimators}.
An estimator $\hat{\btheta}\left(\bd\right)$ is a function of the data $\bd$, that is supposed to approximate the true value of the parameter of interest, which we denote by $\btheta_T$.

The \textbf{bias} of an estimator is given by
\begin{align}
    \label{eq:estimator-bias-definition}
    \mathbb{B}\left[\hat{\btheta}\right]
    &\equiv
    \mathbb{E}\left[\hat{\btheta} - \btheta_T\right]
    \nonumber\\
    &=
    \mathbb{E}\left[\hat{\btheta}\right] - \btheta_T
    .
\end{align}
Note that $\btheta_T$ is an unbiased estimator to $\btheta_T$, which gives us the second line.
An \textbf{unbiased estimator} has zero bias--that is the bias vector has all zero components.
While being unbiased is often useful, biased estimators are often used in practice, as sometimes they can have lower variance than the unbiased estimator (and that can dominate the error of the statistical test).

The sample \textbf{variance} of an estimator is given by
\begin{align}
    \label{eq:estimator-variance-definition}
    \mathbb{V}\left[\hat{\btheta}\right]
    \equiv
    \mathbb{E}\left[\left(\hat{\btheta} - \mathbb{E}\left[\hat{\btheta}\right]\right)^2\right]
    .
\end{align}
Notice that the variance is comparing $\hat{\btheta}$ with its mean. 
This variance essentially measures how much we expect the $\hat{\btheta}$ to vary over different data sets.

Finally, we define the \textbf{mean squared error} of the estimator
\begin{align}
    \label{eq:estimator-mse-definition}
    \mathbb{MSE}\left(\hat{\btheta},\btheta_T\right)
    \equiv
    \mathbb{E}\left[\left(\hat{\btheta} - \btheta_T\right)^2\right]
    .
\end{align}
Unlike the variance, the mean squared error compares the average deviation of the estimator from the true value.

The mean squared error can be written in terms of the bias and variance as follows.
\begin{align}
    \label{eq:estimator-MSE-bais-variance}
    \mathbb{MSE}\left(\hat{\btheta},\btheta_T\right)
    =&
    \mathbb{E}\left[\left(\hat{\btheta} - \btheta_T\right)^2\right]
    \nonumber\\
    =&
    \mathbb{E}\left[\hat{\btheta}^2\right]
    +
    \mathbb{E}\left[\btheta_T^2\right]
    - 
    2\mathbb{E}\left[\hat{\btheta}\btheta_T\right]
    \nonumber\\
    =&
    \left(\mathbb{E}\left[
        \hat{\btheta}^2\right] 
        - 
        \left(\mathbb{E}\left[\hat{\btheta}\right]\right)^2
    \right)
    +
    \left(
        \mathbb{E}\left[\btheta_T^2\right] 
        - 
        \left(\mathbb{E}\left[\btheta_T\right]\right)^2
    \right)
    \nonumber\\
    &
    +
    \left(
        \left(\mathbb{E}\left[\hat{\btheta}\right]\right)^2
        - 
        2\mathbb{E}\left[\hat{\btheta}\btheta_T\right]
        +
        \left(\mathbb{E}\left[\btheta_T\right]\right)^2
    \right)
    \nonumber\\
    =&
    \mathbb{V}\left[\hat{\btheta}\right]
    +
    \mathbb{V}\left[\btheta_T\right]
    +
    \left(\mathbb{E}\left[\hat{\btheta}\right] - \btheta_T\right)^2
    \nonumber\\
    =&
    \mathbb{V}\left[\hat{\btheta}\right]
    +
    \left(\mathbb{B}\left[\hat{\btheta}\right]\right)^2
    +
    \mathbb{V}\left[\btheta_T\right]
    .
\end{align}
We see that the mean squared value of the estimator is equal to the sum of the variance of the estimator, the squared bias of the estimator, and the intrinsic variance of the ``true'' parameter value, $\mathbb{V}\left[\btheta_T\right]$.
This last quantity we can think of a a noise floor--the M.S.E. can never go below that value.
If our measure for how ``good'' an estimator is its M.S.E., then \eqref{eq:estimator-MSE-bais-variance} makes it clear that the bias and variance of the estimator are both important quantities to consider.

%==============================================================================
\section{Hypothesis testing}

\textbf{Hypothesis testing} is arguably a simple form of model comparison.
As traditionally formulated, we have a hypothesis (model), and we want to know how much the data supports the model.
That is, we want to compute $P\left(H_0|\bd\right)$, where $H_0$ is the hypothesis and $d$ is the data.
The default hypothesis $H_0$ is called the \textbf{null hypothesis}.
To perform a hypothesis, we set some value $0<p<1$, and then compute $P\left(H_0|\bd\right)$.
If $P\left(H_0|\bd\right)<p$, we \textbf{reject the null hypothesis}, otherwise we \textbf{accept the null hypothesis}.
The \textbf{alternative hypothesis} to the null hypothesis is called $H_1$, and remains unspecified.

In the frequentist approach to statistics, the null hypothesis is either correct or incorrect.
If the null is correct, then the test can either (correctly) accept the null hypothesis, or (incorrectly) reject it, given a given data set $\bd$.
If the null is incorrect, then the test can either (incorrectly) accept the null hypothesis, or (correctly) reject it, given a given data set $\bd$.

The probability of incorrectly rejecting the null hypothesis when it is true is often denote by $\alpha$, that is
\begin{align}
    \label{eq:frequentist-def-alpha}
    \alpha
    \equiv
    P\left(\mathrm{reject}\;H_0|H_0\;\mathrm{is\;true}\right)
    .
\end{align}
Incorrectly rejecting the null hypothesis is called a \textbf{false positive}, or \textbf{type I error}.
The quantity $1-\alpha$ is called the \textbf{statistical significance} of the test.

The probability of incorrectly accepting the null hypothesis when it is false is often denote by $\beta$, that is
\begin{align}
    \label{eq:frequentist-def-beta}
    \beta
    \equiv
    P\left(\mathrm{accept}\;H_0|H_0\;\mathrm{is\;false}\right)
    .
\end{align}
Incorrectly accepting the null hypothesis is called a \textbf{false negative}, or \textbf{type II error}.
The quantity $1-\beta$ is called the \textbf{statistical power} (or \textbf{sensitivity}) of the test.


%==============================================================================
\section{Confidence intervals}

Here we specialize to scalar statistical quantities for simplicity.
The \textbf{confidence interval} for a parameter $\theta_T$ is an interval $C_n$ such that
\begin{align}
    P\left(\theta_T\in C_n\right) \geq 1 - \alpha
    .
\end{align}
The value $1-\alpha$ is called the \textbf{confidence level} or \textbf{coverage} of the interval.
Here $\alpha$ is defined in \eqref{eq:frequentist-def-alpha}. 

%==============================================================================
\section{$p$-values}
A \textbf{test statistic} $T\left(\bd\right)$ is a scalar function of the data you collect.
Many classical test statistics follow some well-known distribution, given an assumption of the distribution from which the data $\bd$ are drawn from.
In the context of hypothesis testing, good test statistics are sensitive to the nature of the probability distribution you draw data from (the null hypothesis, $H_0$). 

The \textbf{p-value} is the probability that $T$ is greater than or equal to $T\left(\bd\right)$ (the test statistic computed from the observed data), assuming the data are drawn from the null hypothesis $H_0$.
\begin{align}
    \label{eq:p-value-definition}
    p-\mathrm{value}
    \equiv
    P\left(T\left(X\right)\geq T\left(\bd\right)|H_0\right)
    .
\end{align}

